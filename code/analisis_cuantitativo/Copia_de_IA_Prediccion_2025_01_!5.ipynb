{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXON3H17JsL2",
        "outputId": "0075c9d3-e18b-4615-98df-2ad094af90db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from PyWavelets) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "#! pip install matplotlib\n",
        "!pip install PyWavelets\n",
        "import pywt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CODIGO SIN PROBLEMAS POR PARTES**"
      ],
      "metadata": {
        "id": "hWASBEs8KM8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. OHLC Price and Wavelet Denoising"
      ],
      "metadata": {
        "id": "1AbJPc8UKaVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pywt\n",
        "\n",
        "# Cargar datos\n",
        "file_path = '/content/EURUSD_hourly_data_with_sessions.csv'  # Ruta del archivo cargado\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocesar datos\n",
        "data['datetime'] = pd.to_datetime(data['datetime'])\n",
        "data.set_index('datetime', inplace=True)\n",
        "data = data[['open', 'high', 'low', 'close']]\n",
        "\n",
        "# Aplicar transformación Wavelet\n",
        "def denoise_wavelet(data, wavelet='db1', level=1):\n",
        "    coeffs = pywt.wavedec(data, wavelet, mode='symmetric', level=level)\n",
        "    coeffs[-1] = np.zeros_like(coeffs[-1])\n",
        "    coeffs[-2] = np.zeros_like(coeffs[-2])\n",
        "    return pywt.waverec(coeffs, wavelet, mode='symmetric')\n",
        "\n",
        "denoised_close = denoise_wavelet(data['close'].values)\n",
        "data['denoised_close'] = denoised_close"
      ],
      "metadata": {
        "id": "FF217L4kKRBE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Technical Indicators"
      ],
      "metadata": {
        "id": "0EfjoAq7Kka4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular indicadores técnicos\n",
        "data['rsi'] = data['close'].diff().apply(lambda x: max(x, 0)).rolling(window=14).mean() / \\\n",
        "               data['close'].diff().abs().rolling(window=14).mean()\n",
        "data['macd'] = data['close'].ewm(span=12).mean() - data['close'].ewm(span=26).mean()\n",
        "data['signal'] = data['macd'].ewm(span=9).mean()\n"
      ],
      "metadata": {
        "id": "q_A46TXkKpyo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Image Matrix Creation"
      ],
      "metadata": {
        "id": "R6N3aY66Ktyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear matriz temporal (30x30)\n",
        "def create_time_matrix(data, lookback=30):\n",
        "    sequences = []\n",
        "    for i in range(len(data) - lookback):\n",
        "        seq = data[i:i + lookback].values\n",
        "        sequences.append(seq)\n",
        "    return np.array(sequences)\n",
        "\n",
        "matrix_data = create_time_matrix(data[['denoised_close', 'rsi', 'macd', 'signal']], lookback=30)\n",
        "matrix_data = np.expand_dims(matrix_data, axis=-1)  # Expandir para formato de imagen\n"
      ],
      "metadata": {
        "id": "dogL642HKy4V"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. ResNet Feature Extraction"
      ],
      "metadata": {
        "id": "JS4lYjKpK6fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, Add, GlobalAveragePooling2D, Dense, Input\n",
        "\n",
        "# Definir bloques ResNet\n",
        "def resnet_block(input_tensor, filters, kernel_size=3):\n",
        "    x = Conv2D(filters, kernel_size, padding='same')(input_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(filters, kernel_size, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, input_tensor])\n",
        "    x = ReLU()(x)\n",
        "    return x\n",
        "\n",
        "# Construir modelo ResNet\n",
        "def build_resnet(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(64, 3, padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    for _ in range(3):\n",
        "        x = resnet_block(x, 64)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    model = Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "resnet_model = build_resnet(matrix_data.shape[1:])\n",
        "features = resnet_model.predict(matrix_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l2utjGjK_Pp",
        "outputId": "66dd8cee-5e3f-4d2b-df4c-48b9af49cd3c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. LightGBM Training"
      ],
      "metadata": {
        "id": "10HsAApALDRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Simulación de datos para evitar dependencias (reemplaza con tus datos reales)\n",
        "np.random.seed(42)\n",
        "data = pd.DataFrame({\n",
        "    'close': np.random.rand(1000) * 100,\n",
        "    'feature1': np.random.rand(1000),\n",
        "    'feature2': np.random.rand(1000),\n",
        "    'feature3': np.random.rand(1000)\n",
        "})\n",
        "features = data[['feature1', 'feature2', 'feature3']]\n",
        "\n",
        "# Preparar datos para LightGBM\n",
        "labels = pd.cut(data['close'][30:], bins=5, labels=False)  # Categorías discretas\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "y_train = LabelEncoder().fit_transform(y_train)\n",
        "y_test = LabelEncoder().fit_transform(y_test)\n",
        "\n",
        "# Crear dataset LightGBM\n",
        "lgb_train = lgb.Dataset(X_train, y_train)\n",
        "lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
        "\n",
        "# Parámetros y entrenamiento\n",
        "params = {\n",
        "    'objective': 'multiclass',\n",
        "    'metric': 'multi_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 31,\n",
        "    'num_class': len(np.unique(y_train))\n",
        "}\n",
        "\n",
        "# Validación cruzada y entrenamiento\n",
        "cv_results = lgb.cv(\n",
        "    params,\n",
        "    lgb_train,\n",
        "    num_boost_round=500,\n",
        "    callbacks=[\n",
        "        lgb.early_stopping(stopping_rounds=50),\n",
        "        lgb.log_evaluation(10)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Validar las claves disponibles en cv_results\n",
        "print(\"Claves disponibles en cv_results:\")\n",
        "print(cv_results.keys())\n",
        "\n",
        "# Acceder a la clave correcta\n",
        "if 'multi_logloss-mean' in cv_results:\n",
        "    best_iter = len(cv_results['multi_logloss-mean'])\n",
        "elif 'valid_0-multi_logloss-mean' in cv_results:\n",
        "    best_iter = len(cv_results['valid_0-multi_logloss-mean'])\n",
        "else:\n",
        "    raise KeyError(\"No se encontró la métrica 'multi_logloss-mean' en los resultados de la validación cruzada.\")\n",
        "\n",
        "# Entrenar modelo final\n",
        "model = lgb.train(params, lgb_train, num_boost_round=best_iter)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "83qvnYuzLHsw",
        "outputId": "9c302646-dd11-491b-b22b-bc84a6df0fb2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [1000, 970]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-bcb54f5f698b>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Preparar datos para LightGBM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Categorías discretas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2846\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2848\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1000, 970]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Prediction and Evaluation"
      ],
      "metadata": {
        "id": "fAkPFKeBLPWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Predicciones\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Evaluación\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Visualización\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test, label='Real', alpha=0.7)\n",
        "plt.plot(y_pred_classes, label='Predicción', alpha=0.7)\n",
        "plt.title('Predicción vs Realidad (LightGBM + ResNet)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ln8J80KMLTML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.  Métricas de Evaluacion"
      ],
      "metadata": {
        "id": "znTm1LFJNEy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calcula las métricas de calidad para los modelos.\n",
        "\n",
        "    Parámetros:\n",
        "    y_true: array-like\n",
        "        Valores reales.\n",
        "    y_pred: array-like\n",
        "        Valores predichos.\n",
        "\n",
        "    Retorna:\n",
        "    dict\n",
        "        Diccionario con MAE, MSE y RMSE.\n",
        "    \"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    return {\n",
        "        'MAE': mae,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse\n",
        "    }\n",
        "\n",
        "def display_metrics_table(models_metrics):\n",
        "    \"\"\"\n",
        "    Muestra una tabla con las métricas calculadas para diferentes modelos.\n",
        "\n",
        "    Parámetros:\n",
        "    models_metrics: dict\n",
        "        Diccionario con el nombre del modelo como clave y las métricas como valor.\n",
        "\n",
        "    Retorna:\n",
        "    None\n",
        "    \"\"\"\n",
        "    metrics_df = pd.DataFrame(models_metrics).T\n",
        "    metrics_df.columns = ['MAE (×10⁻³)', 'MSE (×10⁻⁶)', 'RMSE (×10⁻³)']\n",
        "    metrics_df.index.name = 'Modelo'\n",
        "    print(metrics_df)\n",
        "\n",
        "# Integración con predicciones de LightGBM y datos finales\n",
        "def run_final_metrics_evaluation(y_test, y_pred):\n",
        "    \"\"\"\n",
        "    Ejecuta la evaluación de métricas al final del ejercicio y muestra los resultados.\n",
        "\n",
        "    Parámetros:\n",
        "    y_test: array-like\n",
        "        Valores reales.\n",
        "    y_pred: dict of array-like\n",
        "        Diccionario con predicciones de los modelos.\n",
        "\n",
        "    Retorna:\n",
        "    None\n",
        "    \"\"\"\n",
        "    models_metrics = {}\n",
        "\n",
        "    for model_name, predictions in y_pred.items():\n",
        "        metrics = calculate_metrics(y_test, predictions)\n",
        "\n",
        "        # Escalar métricas para coincidir con valores reportados\n",
        "        metrics['MAE'] *= 1e3\n",
        "        metrics['MSE'] *= 1e6\n",
        "        metrics['RMSE'] *= 1e3\n",
        "\n",
        "        models_metrics[model_name] = metrics\n",
        "\n",
        "    display_metrics_table(models_metrics)\n",
        "\n",
        "# Ejemplo de uso final con predicciones del modelo LightGBM\n",
        "def evaluate_lgb_model(y_test, y_pred_raw):\n",
        "    \"\"\"\n",
        "    Ajusta las predicciones crudas de LightGBM y evalúa las métricas finales.\n",
        "\n",
        "    Parámetros:\n",
        "    y_test: array-like\n",
        "        Valores reales.\n",
        "    y_pred_raw: array-like\n",
        "        Predicciones crudas del modelo LightGBM (probabilidades).\n",
        "\n",
        "    Retorna:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Convertir probabilidades en clases\n",
        "    y_pred_classes = np.argmax(y_pred_raw, axis=1)\n",
        "\n",
        "    # Ejecutar evaluación\n",
        "    run_final_metrics_evaluation(y_test, {\"LightGBM\": y_pred_classes})\n",
        "\n",
        "# Integración en flujo con LightGBM\n",
        "y_test_example = np.array([0, 1, 2, 1, 0])  # Ejemplo de clases verdaderas\n",
        "y_pred_raw_example = np.array([\n",
        "    [0.7, 0.2, 0.1],\n",
        "    [0.1, 0.8, 0.1],\n",
        "    [0.2, 0.3, 0.5],\n",
        "    [0.2, 0.6, 0.2],\n",
        "    [0.9, 0.05, 0.05]\n",
        "])\n",
        "\n",
        "evaluate_lgb_model(y_test_example, y_pred_raw_example)\n"
      ],
      "metadata": {
        "id": "c3iZ24MJNJCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ENTREGA FINAL 15-01-2024**"
      ],
      "metadata": {
        "id": "-KGBFlqgq1FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pywt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, Add, GlobalAveragePooling2D, Dense, Input\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Instalar dependencias necesarias para Google Colab\n",
        "!pip install pywavelets lightgbm\n",
        "\n",
        "# --------------------------- CONFIGURACIÓN INICIAL ---------------------------\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"\n",
        "    Carga y preprocesa los datos desde un archivo CSV.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Ruta al archivo CSV.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Datos preprocesados.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = pd.read_csv(file_path)\n",
        "        data['datetime'] = pd.to_datetime(data['datetime'])\n",
        "        data.set_index('datetime', inplace=True)\n",
        "        return data[['open', 'high', 'low', 'close']]\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error al cargar o procesar el archivo: {e}\")\n",
        "\n",
        "# --------------------------- PROCESAMIENTO DE SEÑALES ---------------------------\n",
        "def denoise_wavelet(data, wavelet='db1', level=1):\n",
        "    \"\"\"\n",
        "    Aplica desruido de señal utilizando la transformada wavelet.\n",
        "\n",
        "    Args:\n",
        "        data (array-like): Datos de entrada.\n",
        "        wavelet (str): Tipo de wavelet.\n",
        "        level (int): Nivel de descomposición.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Señal suavizada.\n",
        "    \"\"\"\n",
        "    coeffs = pywt.wavedec(data, wavelet, mode='symmetric', level=level)\n",
        "    coeffs[-1] = np.zeros_like(coeffs[-1])\n",
        "    coeffs[-2] = np.zeros_like(coeffs[-2])\n",
        "    return pywt.waverec(coeffs, wavelet, mode='symmetric')\n",
        "\n",
        "# --------------------------- CÁLCULO DE INDICADORES ---------------------------\n",
        "def calculate_technical_indicators(data):\n",
        "    \"\"\"\n",
        "    Calcula indicadores técnicos básicos como RSI y MACD.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): Datos OHLC.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Datos con indicadores añadidos.\n",
        "    \"\"\"\n",
        "    data['rsi'] = data['close'].diff().apply(lambda x: max(x, 0)).rolling(window=14).mean() / \\\n",
        "                   data['close'].diff().abs().rolling(window=14).mean()\n",
        "    data['macd'] = data['close'].ewm(span=12).mean() - data['close'].ewm(span=26).mean()\n",
        "    data['signal'] = data['macd'].ewm(span=9).mean()\n",
        "    return data\n",
        "\n",
        "# --------------------------- CREACIÓN DE MATRICES TEMPORALES ---------------------------\n",
        "def create_time_matrix(data, lookback=30):\n",
        "    \"\"\"\n",
        "    Crea una matriz temporal para análisis secuencial.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): Datos de entrada.\n",
        "        lookback (int): Tamaño de la ventana temporal.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Matriz temporal.\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    for i in range(len(data) - lookback):\n",
        "        seq = data[i:i + lookback].values\n",
        "        sequences.append(seq)\n",
        "    return np.array(sequences)\n",
        "\n",
        "# --------------------------- DEFINICIÓN DEL MODELO RESNET ---------------------------\n",
        "def resnet_block(input_tensor, filters, kernel_size=3):\n",
        "    \"\"\"\n",
        "    Define un bloque ResNet básico.\n",
        "\n",
        "    Args:\n",
        "        input_tensor: Tensor de entrada.\n",
        "        filters (int): Número de filtros.\n",
        "        kernel_size (int): Tamaño del kernel.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Tensor de salida.\n",
        "    \"\"\"\n",
        "    x = Conv2D(filters, kernel_size, padding='same')(input_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(filters, kernel_size, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, input_tensor])\n",
        "    x = ReLU()(x)\n",
        "    return x\n",
        "\n",
        "def build_resnet(input_shape):\n",
        "    \"\"\"\n",
        "    Construye el modelo ResNet.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): Forma de la entrada.\n",
        "\n",
        "    Returns:\n",
        "        Model: Modelo ResNet.\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(64, 3, padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    for _ in range(3):\n",
        "        x = resnet_block(x, 64)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    return Model(inputs, x)\n",
        "\n",
        "# --------------------------- ENTRENAMIENTO LIGHTGBM ---------------------------\n",
        "def train_lightgbm(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Entrena un modelo LightGBM y evalúa su rendimiento.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame): Conjunto de entrenamiento.\n",
        "        y_train (pd.Series): Etiquetas de entrenamiento.\n",
        "        X_test (pd.DataFrame): Conjunto de prueba.\n",
        "        y_test (pd.Series): Etiquetas de prueba.\n",
        "\n",
        "    Returns:\n",
        "        Model: Modelo entrenado.\n",
        "        dict: Resultados de validación cruzada.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        'objective': 'multiclass',\n",
        "        'metric': 'multi_logloss',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': 0.05,\n",
        "        'num_leaves': 31,\n",
        "        'num_class': len(np.unique(y_train))\n",
        "    }\n",
        "\n",
        "    lgb_train = lgb.Dataset(X_train, y_train)\n",
        "    lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
        "\n",
        "    cv_results = lgb.cv(\n",
        "        params,\n",
        "        lgb_train,\n",
        "        num_boost_round=500,\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=50),\n",
        "            lgb.log_evaluation(10)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    best_iter = len(cv_results['multi_logloss-mean'])\n",
        "    model = lgb.train(params, lgb_train, num_boost_round=best_iter)\n",
        "    return model, cv_results\n",
        "\n",
        "# --------------------------- EVALUACIÓN Y MÉTRICAS ---------------------------\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calcula métricas de evaluación para un modelo.\n",
        "\n",
        "    Args:\n",
        "        y_true (array-like): Valores reales.\n",
        "        y_pred (array-like): Valores predichos.\n",
        "\n",
        "    Returns:\n",
        "        dict: Métricas calculadas.\n",
        "    \"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse}\n",
        "\n",
        "# --------------------------- FLUJO PRINCIPAL ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Ruta del archivo (reemplazar con la correcta en Google Colab)\n",
        "    file_path = '/content/EURUSD_hourly_data_with_sessions.csv'\n",
        "\n",
        "    try:\n",
        "        # Carga y preprocesamiento\n",
        "        data = load_and_preprocess_data(file_path)\n",
        "        data['denoised_close'] = denoise_wavelet(data['close'].values)\n",
        "        data = calculate_technical_indicators(data)\n",
        "\n",
        "        # Crear matriz temporal\n",
        "        matrix_data = create_time_matrix(data[['denoised_close', 'rsi', 'macd', 'signal']], lookback=30)\n",
        "        matrix_data = np.expand_dims(matrix_data, axis=-1)\n",
        "\n",
        "        # Modelo ResNet\n",
        "        resnet_model = build_resnet(matrix_data.shape[1:])\n",
        "        features = resnet_model.predict(matrix_data)\n",
        "\n",
        "        # Simulación de datos para LightGBM\n",
        "        labels = pd.cut(data['close'][30:], bins=5, labels=False)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "        model, cv_results = train_lightgbm(X_train, y_train, X_test, y_test)\n",
        "\n",
        "        # Predicciones y evaluación\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        metrics = evaluate_model(y_test, y_pred_classes)\n",
        "\n",
        "        print(f\"Métricas finales: {metrics}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en la ejecución: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miBFgZZOq8cd",
        "outputId": "ab3eff32-5d19-4ff0-a544-cc0495c7f0e6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pywavelets in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from pywavelets) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.13.1)\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 48ms/step\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.156357 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 132136\n",
            "[LightGBM] [Info] Number of data points in the train set: 12780, number of used features: 526\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.165183 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 132136\n",
            "[LightGBM] [Info] Number of data points in the train set: 12781, number of used features: 526\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.192621 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 132136\n",
            "[LightGBM] [Info] Number of data points in the train set: 12781, number of used features: 526\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.218823 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 132136\n",
            "[LightGBM] [Info] Number of data points in the train set: 12781, number of used features: 526\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.204753 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 132136\n",
            "[LightGBM] [Info] Number of data points in the train set: 12781, number of used features: 526\n",
            "[LightGBM] [Info] Start training from score -3.164068\n",
            "[LightGBM] [Info] Start training from score -1.642854\n",
            "[LightGBM] [Info] Start training from score -0.791059\n",
            "[LightGBM] [Info] Start training from score -2.208556\n",
            "[LightGBM] [Info] Start training from score -1.603976\n",
            "[LightGBM] [Info] Start training from score -3.164146\n",
            "[LightGBM] [Info] Start training from score -1.642932\n",
            "[LightGBM] [Info] Start training from score -0.791137\n",
            "[LightGBM] [Info] Start training from score -2.207922\n",
            "[LightGBM] [Info] Start training from score -1.604054\n",
            "[LightGBM] [Info] Start training from score -3.164146\n",
            "[LightGBM] [Info] Start training from score -1.642932\n",
            "[LightGBM] [Info] Start training from score -0.791137\n",
            "[LightGBM] [Info] Start training from score -2.207922\n",
            "[LightGBM] [Info] Start training from score -1.604054\n",
            "[LightGBM] [Info] Start training from score -3.164146\n",
            "[LightGBM] [Info] Start training from score -1.642932\n",
            "[LightGBM] [Info] Start training from score -0.790964\n",
            "[LightGBM] [Info] Start training from score -2.207922\n",
            "[LightGBM] [Info] Start training from score -1.604443\n",
            "[LightGBM] [Info] Start training from score -3.164146\n",
            "[LightGBM] [Info] Start training from score -1.642932\n",
            "[LightGBM] [Info] Start training from score -0.790964\n",
            "[LightGBM] [Info] Start training from score -2.207922\n",
            "[LightGBM] [Info] Start training from score -1.604443\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tcv_agg's valid multi_logloss: 1.30363 + 0.00324982\n",
            "[20]\tcv_agg's valid multi_logloss: 1.26327 + 0.00415121\n",
            "[30]\tcv_agg's valid multi_logloss: 1.235 + 0.00572311\n",
            "[40]\tcv_agg's valid multi_logloss: 1.21413 + 0.00642925\n",
            "[50]\tcv_agg's valid multi_logloss: 1.1966 + 0.00662341\n",
            "[60]\tcv_agg's valid multi_logloss: 1.18298 + 0.00652715\n",
            "[70]\tcv_agg's valid multi_logloss: 1.17075 + 0.00618746\n",
            "[80]\tcv_agg's valid multi_logloss: 1.16021 + 0.00586876\n",
            "[90]\tcv_agg's valid multi_logloss: 1.15032 + 0.0063289\n",
            "[100]\tcv_agg's valid multi_logloss: 1.14116 + 0.00685226\n",
            "[110]\tcv_agg's valid multi_logloss: 1.13264 + 0.00664288\n",
            "[120]\tcv_agg's valid multi_logloss: 1.12482 + 0.0068557\n",
            "[130]\tcv_agg's valid multi_logloss: 1.11735 + 0.00722873\n",
            "[140]\tcv_agg's valid multi_logloss: 1.11034 + 0.00755942\n",
            "[150]\tcv_agg's valid multi_logloss: 1.10372 + 0.00704872\n",
            "[160]\tcv_agg's valid multi_logloss: 1.09764 + 0.00693936\n",
            "[170]\tcv_agg's valid multi_logloss: 1.09178 + 0.00711295\n",
            "[180]\tcv_agg's valid multi_logloss: 1.08625 + 0.00747199\n",
            "[190]\tcv_agg's valid multi_logloss: 1.08104 + 0.007176\n",
            "[200]\tcv_agg's valid multi_logloss: 1.07609 + 0.00729921\n",
            "[210]\tcv_agg's valid multi_logloss: 1.0722 + 0.00743163\n",
            "[220]\tcv_agg's valid multi_logloss: 1.06815 + 0.00792167\n",
            "[230]\tcv_agg's valid multi_logloss: 1.06391 + 0.00847852\n",
            "[240]\tcv_agg's valid multi_logloss: 1.06034 + 0.00864121\n",
            "[250]\tcv_agg's valid multi_logloss: 1.05693 + 0.0089477\n",
            "[260]\tcv_agg's valid multi_logloss: 1.05333 + 0.00944777\n",
            "[270]\tcv_agg's valid multi_logloss: 1.0499 + 0.0094916\n",
            "[280]\tcv_agg's valid multi_logloss: 1.04681 + 0.00969567\n",
            "[290]\tcv_agg's valid multi_logloss: 1.04419 + 0.00960134\n",
            "[300]\tcv_agg's valid multi_logloss: 1.04131 + 0.00990546\n",
            "[310]\tcv_agg's valid multi_logloss: 1.03895 + 0.010093\n",
            "[320]\tcv_agg's valid multi_logloss: 1.03647 + 0.0104315\n",
            "[330]\tcv_agg's valid multi_logloss: 1.03449 + 0.0104624\n",
            "[340]\tcv_agg's valid multi_logloss: 1.03215 + 0.0101926\n",
            "[350]\tcv_agg's valid multi_logloss: 1.03031 + 0.0100771\n",
            "[360]\tcv_agg's valid multi_logloss: 1.02822 + 0.0104264\n",
            "[370]\tcv_agg's valid multi_logloss: 1.02641 + 0.0110735\n",
            "[380]\tcv_agg's valid multi_logloss: 1.02487 + 0.0111153\n",
            "[390]\tcv_agg's valid multi_logloss: 1.02371 + 0.0117406\n",
            "[400]\tcv_agg's valid multi_logloss: 1.02208 + 0.0119022\n",
            "[410]\tcv_agg's valid multi_logloss: 1.02096 + 0.0119797\n",
            "[420]\tcv_agg's valid multi_logloss: 1.01937 + 0.0121381\n",
            "[430]\tcv_agg's valid multi_logloss: 1.01814 + 0.0121078\n",
            "[440]\tcv_agg's valid multi_logloss: 1.01735 + 0.0122272\n",
            "[450]\tcv_agg's valid multi_logloss: 1.01638 + 0.0124542\n",
            "[460]\tcv_agg's valid multi_logloss: 1.01559 + 0.0124034\n",
            "[470]\tcv_agg's valid multi_logloss: 1.0144 + 0.0128457\n",
            "[480]\tcv_agg's valid multi_logloss: 1.01388 + 0.0130731\n",
            "[490]\tcv_agg's valid multi_logloss: 1.01332 + 0.0133469\n",
            "[500]\tcv_agg's valid multi_logloss: 1.01259 + 0.0135941\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tcv_agg's valid multi_logloss: 1.01259 + 0.0135941\n",
            "Error en la ejecución: 'multi_logloss-mean'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KShMv4fXLKle"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}